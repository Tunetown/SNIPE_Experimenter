<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!--NewPage-->
<HTML>
<HEAD>
<!-- Generated by javadoc (build 1.6.0_23) on Fri Mar 16 08:36:27 CET 2012 -->
<TITLE>
NeuralNetwork
</TITLE>

<META NAME="date" CONTENT="2012-03-16">

<LINK REL ="stylesheet" TYPE="text/css" HREF="../../../../stylesheet.css" TITLE="Style">

<SCRIPT type="text/javascript">
function windowTitle()
{
    if (location.href.indexOf('is-external=true') == -1) {
        parent.document.title="NeuralNetwork";
    }
}
</SCRIPT>
<NOSCRIPT>
</NOSCRIPT>

</HEAD>

<BODY BGCOLOR="white" onload="windowTitle();">
<HR>


<!-- ========= START OF TOP NAVBAR ======= -->
<A NAME="navbar_top"><!-- --></A>
<A HREF="#skip-navbar_top" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_top_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="class-use/NeuralNetwork.html"><FONT CLASS="NavBarFont1"><B>Use</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../index-files/index-1.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;PREV CLASS&nbsp;
&nbsp;<A HREF="../../../../com/dkriesel/snipe/core/NeuralNetworkDescriptor.html" title="class in com.dkriesel.snipe.core"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../index.html?com/dkriesel/snipe/core/NeuralNetwork.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="NeuralNetwork.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_top"></A>
<!-- ========= END OF TOP NAVBAR ========= -->

<HR>
<!-- ======== START OF CLASS DATA ======== -->
<H2>
<FONT SIZE="-1">
com.dkriesel.snipe.core</FONT>
<BR>
Class NeuralNetwork</H2>
<PRE>
java.lang.Object
  <IMG SRC="../../../../resources/inherit.gif" ALT="extended by "><B>com.dkriesel.snipe.core.NeuralNetwork</B>
</PRE>
<DL>
<DT><B>All Implemented Interfaces:</B> <DD>java.io.Serializable</DD>
</DL>
<HR>
<DL>
<DT><PRE>public class <B>NeuralNetwork</B><DT>extends java.lang.Object<DT>implements java.io.Serializable</DL>
</PRE>

<P>
<b>Instantiated second using a NeuralNetworkDescriptor instance,</b> this
 class represents the core functionality of SNIPE: A neural network of
 arbitrary topology with a lightweight and calculation efficient data
 structure, the possibility to change the entire network topology as well as
 the synaptic weights and many other features; In the documentation of this
 class, detail information about the efficient data structure of the neural
 network is given, as well as information about its features. Goals of the
 design include:
 
 <ol>
 <li><b>Generalized data structure for arbitrary network topologies</b>, so
 that virtually all network structures can be realized or even easily
 hand-crafted.
 <li><b>Built-In, fast and easy-to-use learning operators</b> for gradient
 descent or evolutionary learning, as well as mechanisms for efficent control
 of even large network populations.
 <li><b>Mechanisms for design and control of even large populations of neural
 networks</b>
 <li><b>Optimal speed neural network data propagation</b> in contrast to naive
 data structures, even in special cases like multi layer perceptrons and
 sparse networks, as well as low computational topology editing effort.
 <li><b>Low memory consumption</b> - grows only with the number of existing
 synapses, not quadratically with the number of neurons
 <li><b>In-situ processing</b> - no extra memory or preprocessing of the data
 structure is necessary in order to use the network after editing
 <li><b>Usage of only low level data structures (arrays)</b> for easy
 portability. It is not the goal to quench the last tiniest bit of
 asymptotical complexity out of the structure, but to make it usable, light
 weight and fast in praxis.
 <li><b>No object-oriented overload</b>, like objects for every neuron or even
 synapses, etc.
 </ol>
 
 
 
 <h1>Fundamentals</h1>
 
 
 
 <p>
 The Neuron enumeration structure is defined as follows: Neuron 0 is the bias
 neuron, at least neuron 1 is an input neuron which may be followed by other
 input neurons, depending on the input dimensionality. The most important
 information considering the neuron index set is, that it is partitioned in
 layers. Neuron 0 is the bias neuron (no layer). From neuron index 1 on, all
 input neurons (layer 0) are enumerated, after that layer 1,2,... and so on.
 The last layer is the output layer. Neurons can be inserted and removed from
 layers. The number of layers per neural net, however, is static once the net
 is initialized.
 
 <p>
 The Neural Network can be parametrized in many ways (neurons, synapses,
 Activation functions, propagation mode, and more) via NeuralNetworkDescriptor
 instances. To create a neural network, you first have to define a descriptor,
 and then create a neural network using the descriptor. Each Networks
 fundamental features are defined by one single descriptor instance, however,
 a descriptor can define many networks' fundamental features. This layout is
 useful for controlling even large populations of networks (which this
 framework allows for). Just read through the descriptor and neuralnetwork
 documentation to get used to the possibilities and operators. For example,
 using the respective methods in the descriptor, you can define if <i>synapse
 classes</i> (for example: forward shortcut synapses) of synapses are allowed
 for usage in your network. Some synapses are _never_ allowed, for example
 those towards the input layer or bias neuron. Have a look at the method
 setSynapse if you want to create particular synapses in your neural network.
 Only synapses that exist will be used for calculating outputs, errors, and
 such.
 
 <p>
 The Neural Networks efficient calculation and lightweight storage complexity
 will be analyzed thoroughly later in the text.
 
 <p>
 In addition, the neural network has an output method for GraphViz code
 (http://www.graphviz.org) in order to visualize.
 
 <p>
 Method names are organized with prefixes and additional succeeding
 information as follows. Methods with prefix <i>clear</i> are used to clear
 some caches that may used by training algorithms or evolutionary operators.
 Methods with prefix <i>count</i> return numbers of different items in the
 network, like countNeurons or countSynapses. Methods with prefix
 <i>create</i> add some structure to the Network or parse the network
 structure from a string (except for synapse, you can create them using
 setSynapse, which either changes the synaptic weight of an existing synapse
 or creates a new one with the given weight). Getter and Setter methods work
 as usual and set and return several different data, like synaptic weight
 values and neuron indices. <i>is</i>-Methods check for the boolean values of
 different statements, for example whether or not a synapse is allowed.
 Methods with prefix <i>mutate</i> perform different kinds of mutation on the
 neural network topology or synaptic weight values that cannot easily be done
 with an invocation of existing methods. For example, adding random non
 existent synapse can easily be done by invoking first
 getSynapseNonExistentAllowedRandom and then setSynapse, therefore there is no
 delete items from the network topology. The prefix <i>train</i> marks methods
 that implement some common training algorithms. The method propagate
 propagates data through the net.
 
 <p>
 Synapses created newly using different operators will be assigned a random
 value dependent on the synapseInitialRange defined in the descriptor.
 
 <p>
 If you read through this javadoc and all regarding methods once, you know
 what can be done with the network.
 
 
 
 
 <h1>Implementation of Lightweight Data Structure</h1>
 
 <p>
 I present a space- and time efficient, light-weight data structure for
 generalized neural networks enabling fast data propagation and arbitrary
 topological changes. The data structure internally only makes use of arrays.
 It doesn't use any high level dynamic data structures. I am aware of the fact
 that some (few) asymptotically complexity classes could be outperformed by
 using structures like hashmaps, but they would in my opinion add a
 significant multiplicative constant to the performance classes and be
 obsolete in reality.
 
 
 <h2>Data Structure Assembly</h2>
 
 
 Let SYNAPSES be the number of synapses and NEURONS the number of neurons in
 the following paragraphs. Nice to know: In the following speed calculations,
 non-existent synapses do not cause the tiniest bit of computational or
 storage effort. All data is kept in native arrays, for wrapping native java
 types like int and double for storage in e.g. array lists would be pretty
 slow.
 
 <p>
 The entire data structure is made of four 2D arrays (one sub-array in every
 array for each neuron): "predecessors", "successors", "predecessorWeights"
 and "successorWeightIndexInPendantPredecessorArray" (yeah, long name, huh? I
 like speaking names). The 2D integer arrays "predecessors" and "successors"
 store the neuron indices of all predecessors and successors <i>in ascending
 order</i>. The 2D double Array "predecessorWeights" stores the synaptic
 weights from each neuron's incoming synapse to its source neuron ordered by
 ascending predecessor index as well. This drives data propagation through the
 net to the maximum possible speed: Only incoming synapses are interesting in
 this case and you don't have to search for synapse weights like for example
 in a naive synapse list implementation.
 
 <p>
 For any Neuron N and any Successor Neuron S, N's
 "successorWeightIndexInPendantPredecessorArray" sub-array, also ordered by
 ascending successor neuron indices, saves the index of the S's
 predecessorWeights sub-array, where the synaptic weight value is stored. This
 seems a bit complicated, but gives us the advantage of storing weights only
 once so we also have to update them only once. This saves search time in many
 cases respectively makes searching obsolete at all.
 
 <p>
 To get you used to the Data structure, I now describe how basic operations
 are performed. In the following description, Capitals (like N) represent
 Neurons. Neuron-related Expressions represent sets of Synapses: N_in
 represents the number of all incoming synapses of Neuron N, N_out analogously
 the number of all outgoing ones. Both N_in and N_out can be as large as
 NEURONS in worst case.
 
 <p>
 Note again, that high-level data structures are intentionally not used. Of
 course, we could squeeze the last bit of asymptotic speed out of the network
 by using hashmaps etc - but we would decrease propagation speed: Even though
 the propagation complexity (for instance) would stay the same, its
 multiplicative constant would be higher because of the data structure
 overhead.
 
 <p>
 The "old data structure" I will sometimes refer to consists of a weight
 matrix and a synapse existence matrix (storage in O(NEURONS^2)). You need a
 matrix like this for the naive storage of neural networks in a such
 generalized way as we do here. Double values are used for synapses (8 byte
 per double) and boolean values for synapse existances in this data structure
 (1 byte per boolean).
 
 
 <h2>Storage Size</h2>
 
 
 Old data stucture: 9*NEURONS^2 bytes. Lightweight structure: 20*SYNAPSES.
 This means that if 45% of the possible synapses (including the illegal ones
 for example towards the input layer or bias) are existent, the lightweight
 data structure has the same size in memory like the old one. This is a pretty
 rare case.
 
 
 <h2>Common Operations are Performed at Maximum Speed Possible</h2>
 
 <p>
 <b>Reading incoming data per neuron N with N_in incoming synapses:</b>
 O(N_in). The Indices of Neurons sending to N are stored in predecessors[N]
 and can therefore be read straight forward with computational effort of
 O(N_in). This is the most important action, as it is performed pretty often
 during the usage of the Neural Net. It can't be performed faster, since every
 incoming synaptic weight must be taken into account. The old matrix data
 structure caused O(NEURONS) of computational effort.
 
 <p>
 <b>Backpropagating per neuron N with N_out outgoing synapses:</b> O(N_out).
 Basically the same principle, but with a little higher multiplicative
 constant for resolving the real weight out of the index (you have to look in
 two arrays instead of one). It can't be performed faster, since every
 outgoing synaptic weight must be taken into account. The old matrix data
 structure caused O(NEURONS) of computational effort.
 
 <p>
 <b>Batch reading and writing all weights (for example for training):</b>
 O(SYNAPSES). You can go through all weights in a linear way if you do it
 either in ascending or descending order within the neuron-based adjacency
 arrays. It can't be performed faster, since every synaptic weight must be
 taken into account. The old matrix data structure caused O(NEURONS^2) of
 computational effort, however had the advantage to allow random access. We
 will get to this aspect later.
 
 
 
 <h2>Other Operations and Structural Changes are Performed Still Fast</h2>
 
 <p>
 <b>Changing a single weight of a synapse from I to J (random access):</b>
 O(log(MIN(I_out,J_in)))). Done by choosing the smaller array of I_out and
 J_in, performing binary search on it, then resolving and changing the weight
 value. In both cases the weight can be changed with constant computational
 effort. The old matrix data structure caused O(1) of computational effort,
 because just one double in the synapse weight matrix had to be resolved and
 altered, so no search is needed.
 
 <p>
 <b>Checking synapse existence / retrieving weight of Synapse from I to J
 (random access): </b> O(log(MIN(I_out,J_in)))). In both cases the weight can
 be retrieved with computational effort analogous to weight change. The old
 matrix data structure caused O(1) of computational effort, because one bit in
 the synapse existence matrix just had to be read or a weight cell had to be
 read.
 
 <p>
 <b>Add particular synapse from I to J:</b> Check if exists. If it does, see
 random access weight change, and you're done. The interesting case is, if it
 doesn't. Them, we have to rewrite successorarrays for I and predecessorarrays
 for J O(I_out+J_in). The altering of predecessorarrays is easy: Just add the
 cell hat represents the synapse at the rightfull (=sorted) place into the
 arrays while rewriting. Altering the successorarrays is similar: Just add
 cells for the synapse to the arrays. Once this is done, you're done with the
 altering regarding the data structures of neurons I and J. But now, here's a
 hazzle: We altered the predecessorarray of neuron j which other
 successorarrays might point to in order to read synaptic weights. Those
 pointers have to be updated, too. In particular implementation, they should
 to be updated before the predecessorarray is, which avoids some neat
 programming errors you can encounter. To do this, just go through the
 predecessor-array-entries <i>behind</i> the place where the new synapse will
 be filled in. Those entries are all shifted by one cell to the ascending
 direction, meaning that their indices will all be increased by one. So you
 need to increase all integers in all
 successorWeightIndexInPendantPredecessorArrays that point to an weight index
 </>behind</i> the insertion point (read twice to get this, but once you're
 done you understood the data structure). So, for every predecessor of J with
 neuron index equal or larger than I's, the successor weight index of the
 synapse pointing to J has to be incremented. In some (rare!) cases, There
 could be O(NEURONS) predecessors of J with a equal or larger index than I,
 for which the search has to be applied. So the worst case of computational
 effort of the entire operation would be O(NEURONS*log(NEURONS)). Obviously,
 J_in will usually be a lot smaller than NEURONS and most likely, we won't
 insert the new synapse in the very first cell of the predecessorArrays, so
 not every single pointer to the following cells needs to be updated. The old
 matrix data structure caused O(1) of computational effort, because just one
 boolean in the synapse existence matrix had to be flipped.
 
 <p>
 <b>Remove synapse from I to J:</b> First: Existence check as above. If
 synapse doesn't exist, you're done. If it does, rewrite successorarray for I
 and predecessorarray for J: O(I_out+J_in). After this, update the successor
 arrays of all neurons with index >= I that have J as an successor (all
 predecessors from J), because J's predecessor array, that stores the
 regarding weights, is decremented from position I on, like above, but vice
 versa. Computational effort of this: O(J_in times searching in the regarding
 predecessor index arrays), worst case each O(NEURONS*log(NEURONS)). As you
 can see, the effort is the same as adding a synapse. The old matrix data
 structure caused O(1) of computational effort, because one bit in the synapse
 existence matrix just had to be flipped.
 
 <p>
 <b>Add a single neuron at index N:</b> Increase Netinputs/Activations arrays
 that are used for processing: O(NEURONS). Enlarge Data Structure (one
 dimension of 2D-arrays): O(NEURONS). Since just references of sub-arrays are
 copied, this computational effort is virtually constant. Increment neuron
 indices in data structure, worst case O(SYNAPSES) if all existing synapses
 are incident to neurons whose index is larger than N, usually much lower. So:
 O(NEURONS+SYNAPSES) worst case. The old matrix data structure would cause an
 computational effort of O(NEURONS^2) (a line and a column in the large 2D
 matrices would have to be added).
 
 <p>
 <b>Remove Neuron N (random access):</b> Remove all Synapses incident to
 neuron (computational effort see above), shrink Data Structure O(NEURONS).
 Since just references of sub-arrays are copied, this computational effort is
 virtually constant. Decrement neuron indices in data structure, worst case
 O(SYNAPSES) if all existing synapses are incident to neurons with index
 larger than N, usually much lower. So, worst case O(SYNAPSES+NEURONS) again.
 The old matrix data structure, like above, caused O(NEURONS^2) of
 computational effort, because the entire matrices had to be rewritten
 removing a line and a column.
 
 
 
 
 <h1>Built-In Gradient-Descent Strategies for Synaptic Weight Modification.</h1>
 
 Those Strategies work using a training sample lesson (see the respective
 class documentation, it's pretty easy). For parametrization of the training
 methods, see the respective method documentation.
 
 <p>
 <b> Backpropagation of Error [MR86]:</b> Implemented in the
 trainBackpropagationOfError() Method. It trains online with a random sample
 order.
 
 <p>
 <b>Resilient Backpropagation [RB94]:</b> Implemented in the
 trainResilientBackpropagation() method. Trains, of course, offline for it
 needs stable gradients. Aditionally, you may decide whether to use the
 improvements of ResilientPropagation published in [Igel2003], which will
 increase the iteration time but may (not: must) yield better results. Thanks
 go to Martin Westhoven for bugfixing Rprop!
 
 
 
 <h1>Built-In Evolution Strategy Operators on Synaptic Weights and Topology</h1>
 
 This class features several evolutionary operators, both altering its
 synaptic weights and topology. Some of those operators need additional memory
 in O(SYNAPSES) for storing for example informations that define how to
 perturbate every weight when the next mutation step is invoked. Those
 additional data structures with data points defined on every weight are built
 similar to the ones storing the weights and topology and are automatically
 size-maintained if topological changes are performed, for example if neurons
 or synapses are added. They are initialized at the point of time the
 respective operator is invoked the first time and independent from each
 other, so you can mutate the net with several mutation operators without
 losing any operator data storage. They are also cloned if the neuralnet is
 cloned, for they may hold data important for next evolution generations. You
 can discard this information for memory reasons invoking the clearCache
 methods. All gaussian random number processes are done by an unsynchronized
 and tuned mersenne twister random number generator provided by the great
 evolutionary framework ECJ (http://cs.gmu.edu/~eclab/projects/ecj/).
 
 <p>
 <b> Operator implementation policy:</b> Only operators, that cannot easily be
 done with an invocation of existing methods from outside (or would be <i>a
 lot</i> slower) are implemented. For example, adding a random non existent
 synapse can easily be done by invoking first
 getSynapseNonExistentAllowedRandom and then setSynapse, therefore there is no
 mutateTopologyCreateRandomSynapse method.
 
 
 <h2>Synaptic Weight Mutation Operators</h2>
 
 The weight change mutators are ordered in ascending complexity. All of their
 method names have the prefix "mutateWeights".
 
 <p>
 <b>Genetic Algorithm style synaptic weight mutation.</b> Uniform-Randomly
 chooses a single synaptic weight and Uniform-Randomly adds or subtract
 exp(x), x Uniform-Randomly chosen out of [-4,1]. Not recommended for usage
 for it converges pretty slow. It's just implemented for testing purposes.
 
 <p>
 <b>Gaussian Mutation</b> [Fogel94]. Adds a gaussian random variable with mean
 0 and a standard-deviation defined by the number of weights to each weight.
 Does not need additional space in memory. Better then the above -Style method
 for some problems, because it does not only move on the axices of the problem
 coordinate system, but still slow.
 
 <p>
 <b>Temperature-Driven Gaussian Mutation</b> [Angeline94]. This operator is
 used by the GNARL Evolution system for recurrent neural networks. Adds a
 gaussian random variable with mean 0 to each weight. Different to the
 standard gaussian Mutation above, the standard deviation is not only defined
 by the number of weights, but also determined by how optimal the neural net
 already solves the given problem. If the network solves the problem quite
 good, only small changes to the weights will be applied, and vice versa .
 Does not need additional space in memory.
 
 <p>
 <b>Perturbation-Vector Driven Adaptive Gaussian Mutation</b> [Fogel94].
 Mutates the weight in the gaussian way as the standard gaussian mutation, but
 in addition, the perturbation strength is evolved in-line per each single
 weight, so the evolution strategy can self-adapt which weights are to train
 stronger. This mutation needs O(SYNAPSES) additional space in memory - one
 double value per synapse. </i> This strategy seems to be the standard for
 weight mutation throughout lots of evolutionary neural network publications I
 read.</i>
 
 
 <h2>Structural Mutation Operators</h2>
 
 <b>Split Neuron</b> [Odri93]: A given inner neuron x is split in two neurons
 y and z, in a way that keeps a strong behavioral link between x and (y and
 z). The new neurons y and z get the same incident synapses as x. Incoming
 synapses' weights are left untouched, and outgoing synapses' weights are
 multiplied by (1+alpha) for neuron y and by (-alpha) for neuron z.
 
 
 <p>
 <b>Split synapse and add Neuron</b> [Stanley02]: Splits a synapse from neuron
 i to neuron k by removing it and inserting another neuron j and two synapses
 from i to j and from j to k.
 
 <h2>Other Features</h2>
 
 <b>Nonconvergent connection significance test </b>[Finnoff93]: Implemented in
 methods with "nonconvergent" in the name. Tests existing and non-existing
 connections as well as existing neurons for significance according to a given
 Data Set so you can determine which connections to create or to delete.
 
 
 
 <h1>References</h1>
 
 <ol>
 <li>[Schwefel81] Schwefel: Numerical Optimization of Computer Models, 1981.
 
 <li>[Fogel94] Fogel: An Introduction to Simulated Evolutionary Optimization,
 IEEE Transactions on Neural Networks Vol 5 No 1, 1994
 
 <li>[Angeline94] Angeline, Saunders, Pollack: An Evolutionary Algorithm that
 constructs recurrent Neural Networks, IEEE Transactions on Neural Networks
 Vol 5 No 1, 1994
 
 <li>[MR86] J. L. McClelland and D. E. Rumelhart. Parallel Distributed
 Processing: Explorations in the Microstructure of Cognition, volume 2. MIT
 Press, Cambridge, 1986.
 
 <li>[RB94] Martin Riedmiller and Heinrich Braun. RPROP – Description and
 Imple- mentation Details. Technical report, University of Karlsruhe, January
 1994.
 
 <li>[Finnoff93] Finnoff 1993. Improving model selection by nonconvergent
 methods. Neural Networks, Vol 6.
 
 <li>[Stanley02] Kenneth O Stanley and Risto Miikkulainen 2002 - Evolving
 Neural Networks through augmenting topologies.
 
 <li>[Yao97] Yao and Lio, A new Evolutionary System for evolving artificial
 neural networks. IEEETrans on NN.
 
 <li>[Odri93] Odri et al, Evolutional Development of a multilevel neural
 network, Neural Networks, Vol 6 no 4 pp 583-595, 1993.
 
 <li>[Igel2003] Igel and Hüsken, Empirical evaluation of the improved Rprop
 learning algorithms, Neurocomputing Vol 50, pp 105--123, Elsevier, 2003.
 
 </ol>
<P>

<P>
<DL>
<DT><B>Version:</B></DT>
  <DD>0.9</DD>
<DT><B>Author:</B></DT>
  <DD>David Kriesel / dkriesel.com</DD>
<DT><B>See Also:</B><DD><A HREF="../../../../serialized-form.html#com.dkriesel.snipe.core.NeuralNetwork">Serialized Form</A></DL>
<HR>

<P>

<!-- ======== CONSTRUCTOR SUMMARY ======== -->

<A NAME="constructor_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Constructor Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#NeuralNetwork(com.dkriesel.snipe.core.NeuralNetworkDescriptor)">NeuralNetwork</A></B>(<A HREF="../../../../com/dkriesel/snipe/core/NeuralNetworkDescriptor.html" title="class in com.dkriesel.snipe.core">NeuralNetworkDescriptor</A>&nbsp;descriptor)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Creates a neural network defined by the input and output layer neuron
 numbers in the given descriptor.</TD>
</TR>
</TABLE>
&nbsp;
<!-- ========== METHOD SUMMARY =========== -->

<A NAME="method_summary"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="2"><FONT SIZE="+2">
<B>Method Summary</B></FONT></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#clearCacheAll()">clearCacheAll</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Clears any cache and shadows.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#clearCacheGaussianMutationAdaptivePertubation()">clearCacheGaussianMutationAdaptivePertubation</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Clears the adaptive perturbation cache used in the
 mutateWeightsGaussianAdaptivePertubationVectorDriven method.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#clearCacheResilientBackpropagation()">clearCacheResilientBackpropagation</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Clears the learning rate, gradient and last weight update caches of
 resilient propagation.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html" title="class in com.dkriesel.snipe.core">NeuralNetwork</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#clone()">clone</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Performs a complete deep copy of this Neural Network and returns it.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countLayers()">countLayers</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of layers of this network with computational effort in
 O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countNeurons()">countNeurons</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of all neurons in this neural network with
 computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countNeuronsInLayer(int)">countNeuronsInLayer</A></B>(int&nbsp;layer)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of neurons in a given layer with computational effort
 in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countNeuronsInner()">countNeuronsInner</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of inner neurons with computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countNeuronsInput()">countNeuronsInput</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Just calls the descriptor's getNumberOfInputNeurons() and returns the
 result with computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countNeuronsOutput()">countNeuronsOutput</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Just calls the descriptor's getNumberOfOutputNeurons() and returns the
 result with computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countNonExistentAllowedSynapses()">countNonExistentAllowedSynapses</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of non existent, even though allowed synapses with
 computational effort in
 O(ALLOWEDSYNAPSES*LOG(SYNAPSES)+NOTALLOWEDSYNAPSES) worst case.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#countSynapses()">countSynapses</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Counts all existing synapses with expected computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createNeuronInLayer(int)">createNeuronInLayer</A></B>(int&nbsp;layer)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adds a neuron to the end of a given layer with computational effort in
 O(NEURONS+SYNAPSES) (worst case).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createNeuronInRandomLayer()">createNeuronInRandomLayer</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adds a neuron to the end of a random inner layer uniformly chosen with
 computational effort in O(NEURONS+SYNAPSES) (worst case).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;double[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createShadowUnmanaged()">createShadowUnmanaged</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createSynapse(int, int, double)">createSynapse</A></B>(int&nbsp;i,
              int&nbsp;j,
              double&nbsp;newWeight)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wraps setSynapse for your convenience - to get more information, read the
 setSynapse documentation.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createSynapsesAllowed()">createSynapsesAllowed</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Adds all allowed synapses to the network topology in an more efficient
 way than adding lots of single ones with the setSynapse() method, with
 expected computational effort O(ALLOWEDSYNAPSES).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createSynapsesFromLayerToLayer(int, int)">createSynapsesFromLayerToLayer</A></B>(int&nbsp;sourceLayer,
                               int&nbsp;targetLayer)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Creates a synapse from every neuron in the source layer to every neuron
 in the target layer, which may be the same, with computational effort
 O(SYNAPSESTOCREATE * SYNAPSECREATIONCOST) (look in the setSynapse
 documentation to learn about the latter).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createSynapsesFromLayerToLayerWithProbability(int, int, double)">createSynapsesFromLayerToLayerWithProbability</A></B>(int&nbsp;sourceLayer,
                                              int&nbsp;targetLayer,
                                              double&nbsp;probability)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Creates a synapse from every neuron in the source layer to every neuron
 in the target layer (which may be the same) -- but only with a given
 probability and with computational effort O(SYNAPSESTOCREATE *
 SYNAPSECREATIONCOST) (look in the setSynapse documentation to learn about
 the latter).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;double[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#createUserShadow(java.lang.String)">createUserShadow</A></B>(java.lang.String&nbsp;key)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Creates a user shadow that is automatically adapted if network topology
 changes in time O(log(NUMBEROFSHADOWS)+log(SYNAPSES)).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#equals(java.lang.Object)">equals</A></B>(java.lang.Object&nbsp;obj)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks if this object equals another.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#existsUserShadow(java.lang.String)">existsUserShadow</A></B>(java.lang.String&nbsp;key)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks whether or not a managed user shadow with the given key exists in
 time O(log(NUMBEROFSHADOWS)).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;java.lang.String</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#exportToString()">exportToString</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This method returns a string description of this neural network that in
 turn can be parsed by the importFromString method.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getActivation(int)">getActivation</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the last activation value of a given neuron.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../com/dkriesel/snipe/core/NeuralNetworkDescriptor.html" title="class in com.dkriesel.snipe.core">NeuralNetworkDescriptor</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getDescriptor()">getDescriptor</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the NeuralNetworkDescriptor instance that was used to create this
 NeuralNetwork.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getLayerOfNeuron(int)">getLayerOfNeuron</A></B>(int&nbsp;neuronNumber)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the layer number from a neuron with computational effort in
 O(LAYERS).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNetInput(int)">getNetInput</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the last net input value of a given neuron.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;<A HREF="../../../../com/dkriesel/snipe/neuronbehavior/NeuronBehavior.html" title="interface in com.dkriesel.snipe.neuronbehavior">NeuronBehavior</A></CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronBehavior(int)">getNeuronBehavior</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gets a Neuron Behavior from a neuron.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronFirstInLayer(int)">getNeuronFirstInLayer</A></B>(int&nbsp;layer)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of the first neuron in the given layer (the one with
 the smallest index) with computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronInnerLeastConnected()">getNeuronInnerLeastConnected</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the inner neuron with the smallest absolute sum of incident
 synapse weights with computational effort in O(SYNAPSES).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronInnerLeastImportantNonConvergentMethod(com.dkriesel.snipe.training.TrainingSampleLesson)">getNeuronInnerLeastImportantNonConvergentMethod</A></B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;set)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Searches through all inner neurons, which is the least important
 according to the sum of incident synapse importance using the
 nonconvergent method of Finnoff 1993.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronInnerLeastReceiving()">getNeuronInnerLeastReceiving</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the inner neuron with the smallest absolute sum of incoming
 synapse weights with computational effort in O(SYNAPSES).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronInnerLeastSending()">getNeuronInnerLeastSending</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the inner neuron with the smallest absolute sum of outgoing
 synapse weights with computational effort in O(SYNAPSES).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronInnerRandom()">getNeuronInnerRandom</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Uniformly selects a random inner neuron with computational effort in
 O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getNeuronLastInLayer(int)">getNeuronLastInLayer</A></B>(int&nbsp;layer)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the number of the last neuron in the given layer (the one with
 the largest index) with computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getRandomDoubleBetweenIncluding(double, double)">getRandomDoubleBetweenIncluding</A></B>(double&nbsp;x,
                                double&nbsp;y)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getRandomIntegerBetweenIncluding(int, int)">getRandomIntegerBetweenIncluding</A></B>(int&nbsp;i,
                                 int&nbsp;j)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getResilientBackpropagationDeltaMax()">getResilientBackpropagationDeltaMax</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getResilientBackpropagationDeltaMin()">getResilientBackpropagationDeltaMin</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getResilientBackpropagationDeltaZero()">getResilientBackpropagationDeltaZero</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getResilientBackpropagationEtaMinus()">getResilientBackpropagationEtaMinus</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getResilientBackpropagationEtaPlus()">getResilientBackpropagationEtaPlus</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getSynapseFeedForwardExistingLeastImportantNonConvergentMethod(com.dkriesel.snipe.training.TrainingSampleLesson)">getSynapseFeedForwardExistingLeastImportantNonConvergentMethod</A></B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;lesson)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Searches the least important of all existing feed forward synapses, using
 the nonconvergent method of Finnoff 1993.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getSynapseFeedForwardNonExistingMostImportantNonConvergentMethod(com.dkriesel.snipe.training.TrainingSampleLesson)">getSynapseFeedForwardNonExistingMostImportantNonConvergentMethod</A></B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;set)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Searches the most important of all not existing feed forward synapses,
 using the nonconvergent method of Finnoff 1993.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getSynapseNonExistentAllowedRandom()">getSynapseNonExistentAllowedRandom</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the neuron indices of a non existing, even though allowed synapse
 selected uniformly random with expected worst case computational effort
 in O(ALLOWEDSYNAPSES*LOG(SYNAPSES)).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getSynapseRandomExistent()">getSynapseRandomExistent</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns the neuron indices of a existing synapse selected uniformly
 random with worst-case computational effort in O(NEURONS).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;double[][]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getUserShadow(java.lang.String)">getUserShadow</A></B>(java.lang.String&nbsp;key)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a managed user shadow with the given key, if existent, or null,
 of not.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getUserShadowValue(int, int, java.lang.String)">getUserShadowValue</A></B>(int&nbsp;i,
                   int&nbsp;j,
                   java.lang.String&nbsp;key)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gets the value in user shadow that is shadowing the synapse from neuron i
 to neuron j in shadow named key with computational effort in
 O(log(NUMBEROFSHADOWS)+log(MIN(I_out,J_in)))).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#getWeight(int, int)">getWeight</A></B>(int&nbsp;i,
          int&nbsp;j)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gets the synaptic weight value from the neuron i to neuron j with
 computational effort in O(log(MIN(I_out,J_in)))).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#hashCode()">hashCode</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hashes the neural network and returns the hash.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#importFromString(java.lang.String)">importFromString</A></B>(java.lang.String&nbsp;description)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
 Parses a Neural Net String built by the exportFromString method and
 builds up the neural net contained.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#isNeuronHidden(int)">isNeuronHidden</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks whether the given neuron is a hidden neuron with computational
 effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#isNeuronInput(int)">isNeuronInput</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks whether the given neuron is an input neuron with computational
 effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#isNeuronOutput(int)">isNeuronOutput</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks whether the given neuron is an output neuron with computational
 effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#isSynapseAllowed(int, int)">isSynapseAllowed</A></B>(int&nbsp;fromNeuron,
                 int&nbsp;toNeuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Checks whether a synapse is allowed with computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;boolean</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#isSynapseExistent(int, int)">isSynapseExistent</A></B>(int&nbsp;i,
                  int&nbsp;j)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns whether a synapse exists with computational effort in
 O(log(MIN(I_out,J_in)))).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mapOutputNeuronToOutputNumber(int)">mapOutputNeuronToOutputNumber</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mapOutputNumberToOutputNeuron(int)">mapOutputNumberToOutputNeuron</A></B>(int&nbsp;outputNumber)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mutateTopologySplitNeuron(int)">mutateTopologySplitNeuron</A></B>(int&nbsp;neuron)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Splits an inner Neuron according to the cell division process in
 [Odri93].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;int</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mutateTopologySplitSynapseAndAddNeuron(int, int)">mutateTopologySplitSynapseAndAddNeuron</A></B>(int&nbsp;i,
                                       int&nbsp;k)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Splits a synapse from neuron i to neuron k by removing it and inserting
 another neuron j and two synapses from i to j and from j to k.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mutateWeightsGAStyle()">mutateWeightsGAStyle</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GA-Style mutation of a single random weight.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mutateWeightsGaussian()">mutateWeightsGaussian</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gaussian Mutation of all weights [Fogel94].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mutateWeightsGaussianAdaptivePertubationVectorDriven()">mutateWeightsGaussianAdaptivePertubationVectorDriven</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perturbation-Vector driven adaptive gaussian mutation (standard)
 [Fogel94].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#mutateWeightsGaussianTemperatureDriven(double)">mutateWeightsGaussianTemperatureDriven</A></B>(double&nbsp;temperature)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Temperature-Driven Gaussian Mutation [Angeline94].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;double[]</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#propagate(double[])">propagate</A></B>(double[]&nbsp;input)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;calculates the output value of the neural net given an input by
 propagating the given input through the network.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#removeNeuron(int)">removeNeuron</A></B>(int&nbsp;neuronToRemove)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removes a neuron at the given position as well as all incident synapses
 with computational effort in O(NEURONS+SYNAPSES) (worst case) if the
 neuron is unconnected, otherwise with additional synapse removal effort.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#removeSynapse(int, int)">removeSynapse</A></B>(int&nbsp;i,
              int&nbsp;j)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removes a synapse from neuron I to neuron J, if existent with
 computational effort in O(NEURONS*log(NEURONS)) (worst case).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#removeSynapsesAndNeuronsAll()">removeSynapsesAndNeuronsAll</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Deletes all synapses and inner neurons from the net with expected
 computational effort in O(1).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#removeSynapsesFromLayerToLayer(int, int)">removeSynapsesFromLayerToLayer</A></B>(int&nbsp;sourceLayer,
                               int&nbsp;targetLayer)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removes synapse that point from every neuron in the source layer to every
 neuron in the target layer, which may be the same, with computational
 effort O(SYNAPSESTOREMOVE * SYNAPSECREATIONCOST) (look in the setSynapse
 documentation to learn about the latter).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#removeSynapsesNotAllowed()">removeSynapsesNotAllowed</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removes synapses, that exist in the network but are not allowed (this may
 happen if a network is created and the allowed synapse types are changed
 in the descriptor afterwards) with expected computational effort in
 O(SYNAPSESTOREMOVE * SYNAPSEREMOVALEFFORT).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#removeUserShadow(java.lang.String)">removeUserShadow</A></B>(java.lang.String&nbsp;key)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Removes a managed user shadow with the given key, if existent, in time
 O(log(NUMBEROFSHADOWS)).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setActivation(int, double)">setActivation</A></B>(int&nbsp;neuron,
              double&nbsp;activation)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the value that will be used as the last activation value of a given
 neuron.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setNeuronBehavior(int, com.dkriesel.snipe.neuronbehavior.NeuronBehavior)">setNeuronBehavior</A></B>(int&nbsp;neuron,
                  <A HREF="../../../../com/dkriesel/snipe/neuronbehavior/NeuronBehavior.html" title="interface in com.dkriesel.snipe.neuronbehavior">NeuronBehavior</A>&nbsp;behavior)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Assigns a Neuron Behavior to a neuron.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setNeuronBehaviors()">setNeuronBehaviors</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Retrieves the neuron behaviors from the descriptor and assigns them to
 neurons.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setResilientBackpropagationDeltaMax(double)">setResilientBackpropagationDeltaMax</A></B>(double&nbsp;resilientBackpropagationDeltaMax)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the resilientBackpropagationDeltaMax.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setResilientBackpropagationDeltaMin(double)">setResilientBackpropagationDeltaMin</A></B>(double&nbsp;resilientBackpropagationDeltaMin)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the resilientBackpropagationDeltaMin.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setResilientBackpropagationDeltaZero(double)">setResilientBackpropagationDeltaZero</A></B>(double&nbsp;resilientBackpropagationDeltaZero)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the resilientBackpropagationDeltaZero.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setResilientBackpropagationEtaMinus(double)">setResilientBackpropagationEtaMinus</A></B>(double&nbsp;resilientBackpropagationEtaMinus)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the resilientBackpropagationEtaMinus.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setResilientBackpropagationEtaPlus(double)">setResilientBackpropagationEtaPlus</A></B>(double&nbsp;resilientBackpropagationEtaPlus)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the resilientBackpropagationEtaPlus.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setSynapse(int, int, double)">setSynapse</A></B>(int&nbsp;i,
           int&nbsp;j,
           double&nbsp;newWeight)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Updates or randomly chooses the synaptic weight value from the neuron i
 to neuron j (computational effort: O(log(MIN(I_out,J_in))))) - If the
 synapse doesn't exist, it is added, if it is allowed (worst case
 computational effort: O(NEURONS*log(NEURONS))).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>protected &nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#setUserShadowValue(int, int, java.lang.String, double)">setUserShadowValue</A></B>(int&nbsp;i,
                   int&nbsp;j,
                   java.lang.String&nbsp;key,
                   double&nbsp;value)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sets the value in user shadow that is shadowing the synapse from neuron i
 to neuron j in shadow named key with computational effort in
 O(log(NUMBEROFSHADOWS)+log(MIN(I_out,J_in)))).</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;java.lang.String</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#toString()">toString</A></B>()</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Returns a very simple string representation just printing out the neurons
 per layer structure of the neural net and the number of synapses.</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#trainBackpropagationOfError(com.dkriesel.snipe.training.TrainingSampleLesson, int, double)">trainBackpropagationOfError</A></B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;lesson,
                            int&nbsp;runs,
                            double&nbsp;eta)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Trains the Neural Network with the Training Method "Backpropagation of
 Error" [MR86].</TD>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD ALIGN="right" VALIGN="top" WIDTH="1%"><FONT SIZE="-1">
<CODE>&nbsp;void</CODE></FONT></TD>
<TD><CODE><B><A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html#trainResilientBackpropagation(com.dkriesel.snipe.training.TrainingSampleLesson, int, boolean)">trainResilientBackpropagation</A></B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;lesson,
                              int&nbsp;runs,
                              boolean&nbsp;improvedRprop)</CODE>

<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Trains the Neural Network with the Training Method "Resilient
 Backpropagation of Error" [RB94].</TD>
</TR>
</TABLE>
&nbsp;<A NAME="methods_inherited_from_class_java.lang.Object"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#EEEEFF" CLASS="TableSubHeadingColor">
<TH ALIGN="left"><B>Methods inherited from class java.lang.Object</B></TH>
</TR>
<TR BGCOLOR="white" CLASS="TableRowColor">
<TD><CODE>finalize, getClass, notify, notifyAll, wait, wait, wait</CODE></TD>
</TR>
</TABLE>
&nbsp;
<P>

<!-- ========= CONSTRUCTOR DETAIL ======== -->

<A NAME="constructor_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Constructor Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="NeuralNetwork(com.dkriesel.snipe.core.NeuralNetworkDescriptor)"><!-- --></A><H3>
NeuralNetwork</H3>
<PRE>
public <B>NeuralNetwork</B>(<A HREF="../../../../com/dkriesel/snipe/core/NeuralNetworkDescriptor.html" title="class in com.dkriesel.snipe.core">NeuralNetworkDescriptor</A>&nbsp;descriptor)</PRE>
<DL>
<DD>Creates a neural network defined by the input and output layer neuron
 numbers in the given descriptor. Uses the same initialization values as
 the array-based constructor. Does the same as calling
 createNeuralNetwork() from the descriptor.
<P>
<DL>
<DT><B>Parameters:</B><DD><CODE>descriptor</CODE> - the descriptor defining some constraints on the NeuralNetwork.</DL>
</DL>

<!-- ============ METHOD DETAIL ========== -->

<A NAME="method_detail"><!-- --></A>
<TABLE BORDER="1" WIDTH="100%" CELLPADDING="3" CELLSPACING="0" SUMMARY="">
<TR BGCOLOR="#CCCCFF" CLASS="TableHeadingColor">
<TH ALIGN="left" COLSPAN="1"><FONT SIZE="+2">
<B>Method Detail</B></FONT></TH>
</TR>
</TABLE>

<A NAME="mutateWeightsGAStyle()"><!-- --></A><H3>
mutateWeightsGAStyle</H3>
<PRE>
public void <B>mutateWeightsGAStyle</B>()</PRE>
<DL>
<DD>GA-Style mutation of a single random weight. Uniform-Randomly chooses a
 single synaptic weight (if at least one synapse exists) and
 Uniform-Randomly adds or subtracts exp(x), x Uniform-Randomly chosen out
 of [-4,1]. Not recommended for usage for it converges pretty slow.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="mutateWeightsGaussian()"><!-- --></A><H3>
mutateWeightsGaussian</H3>
<PRE>
public void <B>mutateWeightsGaussian</B>()</PRE>
<DL>
<DD>Gaussian Mutation of all weights [Fogel94]. Adds a gaussian random
 variable with mean 0 and a standard-deviation 1.0 / (Math.sqrt(2 * Math
 .sqrt(numberOfSynapses))) to each weight. Does not need additional space
 in memory. Better than the GA-Style method for some problems, because it
 does not only move on the axices of the problem coordinate system, but
 still slow.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="mutateWeightsGaussianTemperatureDriven(double)"><!-- --></A><H3>
mutateWeightsGaussianTemperatureDriven</H3>
<PRE>
public void <B>mutateWeightsGaussianTemperatureDriven</B>(double&nbsp;temperature)</PRE>
<DL>
<DD>Temperature-Driven Gaussian Mutation [Angeline94]. This operator is used
 by the GNARL Evolution system for recurrent neural networks. Adds a
 gaussian random variable with mean 0 to each weight. Different to the
 standard gaussian Mutation above, the standard deviation is not only
 defined by the number of weights, but also determined by how optimal the
 neural net already solves the given problem. If the network solves the
 problem quite good, only small changes to the weights can be applied, and
 vice versa. Does not need additional space in memory. It is recommended
 to calculate the temperature directly by the network error on a given
 training set. The temperature is directly multiplied with the gaussian to
 add to each weight. The result is then multiplied with another random
 variable uniformly chosen of [0,1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>temperature</CODE> - Has to be near 1 if large mutations should be performed, and
            near 0, if only small mutations are desired.</DL>
</DD>
</DL>
<HR>

<A NAME="mutateWeightsGaussianAdaptivePertubationVectorDriven()"><!-- --></A><H3>
mutateWeightsGaussianAdaptivePertubationVectorDriven</H3>
<PRE>
public void <B>mutateWeightsGaussianAdaptivePertubationVectorDriven</B>()</PRE>
<DL>
<DD>Perturbation-Vector driven adaptive gaussian mutation (standard)
 [Fogel94]. Mutates the weight in the gaussian way as the standard
 gaussian mutation, but in addition, the perturbation strength is evolved
 in-line per each single weight, so the evolution strategy can self-adapt
 which weights are to train stronger. This mutation needs O(SYNAPSES)
 additional space in memory - one double value per synapse. <i> Seems to
 be standard method over lots of evolutionary neural networks papers.</i>
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="getRandomDoubleBetweenIncluding(double, double)"><!-- --></A><H3>
getRandomDoubleBetweenIncluding</H3>
<PRE>
protected double <B>getRandomDoubleBetweenIncluding</B>(double&nbsp;x,
                                                 double&nbsp;y)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>x</CODE> - <DD><CODE>y</CODE> - 
<DT><B>Returns:</B><DD>a double chosen uniformly at random between x and y.</DL>
</DD>
</DL>
<HR>

<A NAME="createSynapsesAllowed()"><!-- --></A><H3>
createSynapsesAllowed</H3>
<PRE>
public void <B>createSynapsesAllowed</B>()</PRE>
<DL>
<DD>Adds all allowed synapses to the network topology in an more efficient
 way than adding lots of single ones with the setSynapse() method, with
 expected computational effort O(ALLOWEDSYNAPSES). The synaptic weights
 are chosen uniformly by random within the range defined in the
 descriptor. Exact computational effort:
 O(epsilon*NEURONS^2+ALLOWEDSYNAPSES), with a <i>very</i> small epsilon.
 
 <p>
 The synapses will be assigned a random value dependent on the
 synapseInitialRange defined in the descriptor. TODO hier uu den Fan in
 nehmen oder spezielle funktion dafür schreiben
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="clearCacheAll()"><!-- --></A><H3>
clearCacheAll</H3>
<PRE>
public void <B>clearCacheAll</B>()</PRE>
<DL>
<DD>Clears any cache and shadows.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="clearCacheResilientBackpropagation()"><!-- --></A><H3>
clearCacheResilientBackpropagation</H3>
<PRE>
public void <B>clearCacheResilientBackpropagation</B>()</PRE>
<DL>
<DD>Clears the learning rate, gradient and last weight update caches of
 resilient propagation.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="clearCacheGaussianMutationAdaptivePertubation()"><!-- --></A><H3>
clearCacheGaussianMutationAdaptivePertubation</H3>
<PRE>
public void <B>clearCacheGaussianMutationAdaptivePertubation</B>()</PRE>
<DL>
<DD>Clears the adaptive perturbation cache used in the
 mutateWeightsGaussianAdaptivePertubationVectorDriven method.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="removeSynapsesAndNeuronsAll()"><!-- --></A><H3>
removeSynapsesAndNeuronsAll</H3>
<PRE>
public void <B>removeSynapsesAndNeuronsAll</B>()</PRE>
<DL>
<DD>Deletes all synapses and inner neurons from the net with expected
 computational effort in O(1). Exact computational effort is in
 O(EPSILON*NEURONS), where EPSILON is very small. Unlike removing every
 single synapse and neuron via the removeSynapse or -Neurons method, this
 method runs very fast by just re-initializing the data structure.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="setNeuronBehaviors()"><!-- --></A><H3>
setNeuronBehaviors</H3>
<PRE>
public void <B>setNeuronBehaviors</B>()</PRE>
<DL>
<DD>Retrieves the neuron behaviors from the descriptor and assigns them to
 neurons.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronFirstInLayer(int)"><!-- --></A><H3>
getNeuronFirstInLayer</H3>
<PRE>
public int <B>getNeuronFirstInLayer</B>(int&nbsp;layer)</PRE>
<DL>
<DD>Returns the number of the first neuron in the given layer (the one with
 the smallest index) with computational effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>layer</CODE> - 
<DT><B>Returns:</B><DD>the number of the first neuron in the layer.</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronLastInLayer(int)"><!-- --></A><H3>
getNeuronLastInLayer</H3>
<PRE>
public int <B>getNeuronLastInLayer</B>(int&nbsp;layer)</PRE>
<DL>
<DD>Returns the number of the last neuron in the given layer (the one with
 the largest index) with computational effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>layer</CODE> - 
<DT><B>Returns:</B><DD>the number of the last neuron in the layer.</DL>
</DD>
</DL>
<HR>

<A NAME="propagate(double[])"><!-- --></A><H3>
propagate</H3>
<PRE>
public double[] <B>propagate</B>(double[]&nbsp;input)</PRE>
<DL>
<DD>calculates the output value of the neural net given an input by
 propagating the given input through the network. There are two methods of
 propagation: the normal mode and the fast mode. Which mode is taken, is
 defined by the frequency.
 
 <p>
 In normal mode (which is denoted by any frequency-number greater or equal
 1), all neurons first calculate their net input collecting data from
 their incoming connections. Afterwards, they calculate their activation
 values by propagating their net input through their activation function.
 So virtually, all neurons change states in parallel, network time steps
 do not overlap, which is called "synchronous update". They do so the a
 number of times equal to the frequency. So, a frequency of 10 makes the
 network propagate the data 10 times. A frequency of 1 is the default
 value. This is the natural way for generalized recurrent networks.
 However, for example, a feed forward network with three weight layers
 would need a frequency of 3 in order propagate data through the entire
 network. In such cases, fastprop saves a lot of time.
 
 <p>
 In the fastprop mode (which is denoted by frequency 0) activations are
 directly updated, along with the net inputs which causes the activation
 order of the neurons to be relevant. Any neuron whose activation is
 calculated will immediately use the new activation values of neurons with
 indices smaller (asynchronous update). For feedforward networks, the
 propagation effort is divided by the number of synapse layers while
 getting the same results. For networks with recurrent connections
 allowed, this propagation mode will throw exceptions.
 
 <p>
 Computational Effort: O(SYNAPSES) per single propagation.
 
 TODO was passiert, wenn ein Netz z.b. backwardsynapsen hat, diese aber
 später im descriptor als unerlaubt markiert werden? Hier müsste es ein
 pruning geben
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>input</CODE> - 
<DT><B>Returns:</B><DD>the output @ if the lenght the input is wrong.</DL>
</DD>
</DL>
<HR>

<A NAME="createSynapse(int, int, double)"><!-- --></A><H3>
createSynapse</H3>
<PRE>
public void <B>createSynapse</B>(int&nbsp;i,
                          int&nbsp;j,
                          double&nbsp;newWeight)</PRE>
<DL>
<DD>Wraps setSynapse for your convenience - to get more information, read the
 setSynapse documentation.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - <DD><CODE>j</CODE> - <DD><CODE>newWeight</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="setSynapse(int, int, double)"><!-- --></A><H3>
setSynapse</H3>
<PRE>
public void <B>setSynapse</B>(int&nbsp;i,
                       int&nbsp;j,
                       double&nbsp;newWeight)</PRE>
<DL>
<DD>Updates or randomly chooses the synaptic weight value from the neuron i
 to neuron j (computational effort: O(log(MIN(I_out,J_in))))) - If the
 synapse doesn't exist, it is added, if it is allowed (worst case
 computational effort: O(NEURONS*log(NEURONS))).
 <p>
 If newWeight is assigned the value Double.NaN, the synapse will be
 assigned a random value dependent on the synapseInitialRange defined in
 the descriptor.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - the start neuron of the synapse<DD><CODE>j</CODE> - the end neuron of the synapse<DD><CODE>newWeight</CODE> - If assigned the value Double.NaN, the synapse will be assigned
            a random value dependent on the synapseInitialRange defined in
            the descriptor.</DL>
</DD>
</DL>
<HR>

<A NAME="removeSynapsesNotAllowed()"><!-- --></A><H3>
removeSynapsesNotAllowed</H3>
<PRE>
public void <B>removeSynapsesNotAllowed</B>()</PRE>
<DL>
<DD>Removes synapses, that exist in the network but are not allowed (this may
 happen if a network is created and the allowed synapse types are changed
 in the descriptor afterwards) with expected computational effort in
 O(SYNAPSESTOREMOVE * SYNAPSEREMOVALEFFORT).
 
 <p>
 Exact computational effort: O(epsilon * SYNAPSES + SYNAPSESTOREMOVE *
 SYNAPSEREMOVALEFFORT), where epsilon is very small and SYNAPSESTOREMOVE
 denotes the number of synapses that are actually removed. To learn about
 SYNAPSEREMOVALEFFORT, please check the documentation of the removeSynapse
 method.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
</DL>
</DD>
</DL>
<HR>

<A NAME="removeSynapse(int, int)"><!-- --></A><H3>
removeSynapse</H3>
<PRE>
public void <B>removeSynapse</B>(int&nbsp;i,
                          int&nbsp;j)</PRE>
<DL>
<DD>Removes a synapse from neuron I to neuron J, if existent with
 computational effort in O(NEURONS*log(NEURONS)) (worst case). If synapse
 doesn't exist, the computational effort is reduced to the synapse
 existence check.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - <DD><CODE>j</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="isSynapseExistent(int, int)"><!-- --></A><H3>
isSynapseExistent</H3>
<PRE>
public boolean <B>isSynapseExistent</B>(int&nbsp;i,
                                 int&nbsp;j)</PRE>
<DL>
<DD>Returns whether a synapse exists with computational effort in
 O(log(MIN(I_out,J_in)))).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - the start neuron of the synapse<DD><CODE>j</CODE> - the end neuron of the synapse
<DT><B>Returns:</B><DD>if the synapse exists depending on if it is switched on and if it
         is allowed @ if i and j are illegal chosen in some way</DL>
</DD>
</DL>
<HR>

<A NAME="getWeight(int, int)"><!-- --></A><H3>
getWeight</H3>
<PRE>
public double <B>getWeight</B>(int&nbsp;i,
                        int&nbsp;j)</PRE>
<DL>
<DD>Gets the synaptic weight value from the neuron i to neuron j with
 computational effort in O(log(MIN(I_out,J_in)))).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - the start neuron of the synapse<DD><CODE>j</CODE> - the end neuron of the synapse
<DT><B>Returns:</B><DD>the weight value.</DL>
</DD>
</DL>
<HR>

<A NAME="getUserShadowValue(int, int, java.lang.String)"><!-- --></A><H3>
getUserShadowValue</H3>
<PRE>
protected double <B>getUserShadowValue</B>(int&nbsp;i,
                                    int&nbsp;j,
                                    java.lang.String&nbsp;key)</PRE>
<DL>
<DD>Gets the value in user shadow that is shadowing the synapse from neuron i
 to neuron j in shadow named key with computational effort in
 O(log(NUMBEROFSHADOWS)+log(MIN(I_out,J_in)))).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - the start neuron of the synapse that is shadowed<DD><CODE>j</CODE> - the end neuron of the synapse that is shadowed<DD><CODE>key</CODE> - the user shadow key
<DT><B>Returns:</B><DD>the shadow value.</DL>
</DD>
</DL>
<HR>

<A NAME="setUserShadowValue(int, int, java.lang.String, double)"><!-- --></A><H3>
setUserShadowValue</H3>
<PRE>
protected void <B>setUserShadowValue</B>(int&nbsp;i,
                                  int&nbsp;j,
                                  java.lang.String&nbsp;key,
                                  double&nbsp;value)</PRE>
<DL>
<DD>Sets the value in user shadow that is shadowing the synapse from neuron i
 to neuron j in shadow named key with computational effort in
 O(log(NUMBEROFSHADOWS)+log(MIN(I_out,J_in)))).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - the start neuron of the synapse that is shadowed<DD><CODE>j</CODE> - the end neuron of the synapse that is shadowed<DD><CODE>key</CODE> - the user shadow key<DD><CODE>value</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="mapOutputNumberToOutputNeuron(int)"><!-- --></A><H3>
mapOutputNumberToOutputNeuron</H3>
<PRE>
protected int <B>mapOutputNumberToOutputNeuron</B>(int&nbsp;outputNumber)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>outputNumber</CODE> - output number to map to neuron number
<DT><B>Returns:</B><DD>neuron number</DL>
</DD>
</DL>
<HR>

<A NAME="mapOutputNeuronToOutputNumber(int)"><!-- --></A><H3>
mapOutputNeuronToOutputNumber</H3>
<PRE>
protected int <B>mapOutputNeuronToOutputNumber</B>(int&nbsp;neuron)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - neuron number to map to output number
<DT><B>Returns:</B><DD>output number</DL>
</DD>
</DL>
<HR>

<A NAME="trainBackpropagationOfError(com.dkriesel.snipe.training.TrainingSampleLesson, int, double)"><!-- --></A><H3>
trainBackpropagationOfError</H3>
<PRE>
public void <B>trainBackpropagationOfError</B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;lesson,
                                        int&nbsp;runs,
                                        double&nbsp;eta)</PRE>
<DL>
<DD>Trains the Neural Network with the Training Method "Backpropagation of
 Error" [MR86]. If other connections than forward and forward shortcuts
 are allowed, an exception will be thrown. It trains online with a random
 sample order.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>lesson</CODE> - the training lesson to learn<DD><CODE>runs</CODE> - The number of random patterns to train.<DD><CODE>eta</CODE> - The learning rate.</DL>
</DD>
</DL>
<HR>

<A NAME="getRandomIntegerBetweenIncluding(int, int)"><!-- --></A><H3>
getRandomIntegerBetweenIncluding</H3>
<PRE>
protected int <B>getRandomIntegerBetweenIncluding</B>(int&nbsp;i,
                                               int&nbsp;j)</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - <DD><CODE>j</CODE> - 
<DT><B>Returns:</B><DD>a random integer in the interval [i,j].</DL>
</DD>
</DL>
<HR>

<A NAME="createUserShadow(java.lang.String)"><!-- --></A><H3>
createUserShadow</H3>
<PRE>
protected double[][] <B>createUserShadow</B>(java.lang.String&nbsp;key)</PRE>
<DL>
<DD>Creates a user shadow that is automatically adapted if network topology
 changes in time O(log(NUMBEROFSHADOWS)+log(SYNAPSES)). If the key entered
 is already occupied, an IllegalArgumentException is thrown. <b>Be aware
 of the fact that if synapses are added to the network, the corresponding
 new shadow fields are initialized with 0!</b> If this behavior has
 negative influence on how you use the shadows, you need to keep track of
 the topological changes in the network yourself and handle them
 accordingly.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>key</CODE> - the key to identify the shadow later on.
<DT><B>Returns:</B><DD>the new user shadow.</DL>
</DD>
</DL>
<HR>

<A NAME="createShadowUnmanaged()"><!-- --></A><H3>
createShadowUnmanaged</H3>
<PRE>
protected double[][] <B>createShadowUnmanaged</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>a shadow for local use that is <i>not</i> automatically adapted
         if network topology changes in time O(log(SYNAPSES)).</DL>
</DD>
</DL>
<HR>

<A NAME="removeUserShadow(java.lang.String)"><!-- --></A><H3>
removeUserShadow</H3>
<PRE>
protected void <B>removeUserShadow</B>(java.lang.String&nbsp;key)</PRE>
<DL>
<DD>Removes a managed user shadow with the given key, if existent, in time
 O(log(NUMBEROFSHADOWS)). This shadow is no longer maintained if topology
 changes and will be subject to deletion by garbage collection if not
 referenced elsewhere.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>key</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="getUserShadow(java.lang.String)"><!-- --></A><H3>
getUserShadow</H3>
<PRE>
protected double[][] <B>getUserShadow</B>(java.lang.String&nbsp;key)</PRE>
<DL>
<DD>Returns a managed user shadow with the given key, if existent, or null,
 of not. Takes time O(log(NUMBEROFSHADOWS)). No clone but the original
 reference stored in the shadow data structure is returned, so if
 something within the shadow is changed, the changes will be accessible
 via getUserShadow later on.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>key</CODE> - 
<DT><B>Returns:</B><DD>the user shadow</DL>
</DD>
</DL>
<HR>

<A NAME="existsUserShadow(java.lang.String)"><!-- --></A><H3>
existsUserShadow</H3>
<PRE>
protected boolean <B>existsUserShadow</B>(java.lang.String&nbsp;key)</PRE>
<DL>
<DD>Checks whether or not a managed user shadow with the given key exists in
 time O(log(NUMBEROFSHADOWS)).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>key</CODE> - 
<DT><B>Returns:</B><DD>whether or not a managed user shadow with the given key exists.</DL>
</DD>
</DL>
<HR>

<A NAME="trainResilientBackpropagation(com.dkriesel.snipe.training.TrainingSampleLesson, int, boolean)"><!-- --></A><H3>
trainResilientBackpropagation</H3>
<PRE>
public void <B>trainResilientBackpropagation</B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;lesson,
                                          int&nbsp;runs,
                                          boolean&nbsp;improvedRprop)</PRE>
<DL>
<DD>Trains the Neural Network with the Training Method "Resilient
 Backpropagation of Error" [RB94]. If other connections than forward or
 forward shortcuts are allowed, an exception will be thrown. It trains
 offline for it needs stable gradients in order to work. The initial
 weight updates are set to 0.1, the maximum weight updates to 50 as
 proposed in [RB94]. The method permanently caches synaptic weight
 learning rates, last weight updates and gradients with a storage effort
 of O(SYNAPSES). Some additional caching in O(SYNAPSES) is done during the
 execution of the method. This cache is maintained even after invocation
 of this method in case training will continue later. If you like, use the
 respective clear method to clear those caches. A boolean parameter lets
 you decide to use the improvements of ResilientPropagation published in
 [Igel2003], which will increase the iteration time but may (not: must)
 yield better results. In the context of this method, special thanks go to
 Martin Westhoven for bugfixing and testing!
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>lesson</CODE> - the training lesson to learn.<DD><CODE>runs</CODE> - The number of iterations.<DD><CODE>improvedRprop</CODE> - If improved Rprop after [Igel2003] shall be used</DL>
</DD>
</DL>
<HR>

<A NAME="isNeuronOutput(int)"><!-- --></A><H3>
isNeuronOutput</H3>
<PRE>
public boolean <B>isNeuronOutput</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Checks whether the given neuron is an output neuron with computational
 effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - 
<DT><B>Returns:</B><DD>if the given neuron is an output neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="isNeuronHidden(int)"><!-- --></A><H3>
isNeuronHidden</H3>
<PRE>
public boolean <B>isNeuronHidden</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Checks whether the given neuron is a hidden neuron with computational
 effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - 
<DT><B>Returns:</B><DD>if the given neuron is a hidden neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="getSynapseRandomExistent()"><!-- --></A><H3>
getSynapseRandomExistent</H3>
<PRE>
public int[] <B>getSynapseRandomExistent</B>()</PRE>
<DL>
<DD>Returns the neuron indices of a existing synapse selected uniformly
 random with worst-case computational effort in O(NEURONS). If no such
 synapse exists, null is returned.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the neuron indices of a existing synapse selected uniformly
         random. result[0] is the predecessor, result[1] the successor.
         Returns null if no such synapse exists.</DL>
</DD>
</DL>
<HR>

<A NAME="getSynapseNonExistentAllowedRandom()"><!-- --></A><H3>
getSynapseNonExistentAllowedRandom</H3>
<PRE>
public int[] <B>getSynapseNonExistentAllowedRandom</B>()</PRE>
<DL>
<DD>Returns the neuron indices of a non existing, even though allowed synapse
 selected uniformly random with expected worst case computational effort
 in O(ALLOWEDSYNAPSES*LOG(SYNAPSES)). Exact computational effort is
 O(ALLOWEDSYNAPSES*LOG(SYNAPSES)+EPSILON*NOTALLOWEDSYNAPSES) in worst
 case, where EPSILON is very small. If no such synapse exists, null is
 returned.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the neuron indices of a existing synapse selected uniformly
         random. result[0] is the predecessor, result[1] the successor.
         Returns null of no such synapse exists.</DL>
</DD>
</DL>
<HR>

<A NAME="getSynapseFeedForwardExistingLeastImportantNonConvergentMethod(com.dkriesel.snipe.training.TrainingSampleLesson)"><!-- --></A><H3>
getSynapseFeedForwardExistingLeastImportantNonConvergentMethod</H3>
<PRE>
public int[] <B>getSynapseFeedForwardExistingLeastImportantNonConvergentMethod</B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;lesson)</PRE>
<DL>
<DD>Searches the least important of all existing feed forward synapses, using
 the nonconvergent method of Finnoff 1993.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>lesson</CODE> - the lesson to measure
<DT><B>Returns:</B><DD>the least important existing feed forward connection according to
         finnoff's nonconvergent method or null if no synapse exists.</DL>
</DD>
</DL>
<HR>

<A NAME="createSynapsesFromLayerToLayer(int, int)"><!-- --></A><H3>
createSynapsesFromLayerToLayer</H3>
<PRE>
public void <B>createSynapsesFromLayerToLayer</B>(int&nbsp;sourceLayer,
                                           int&nbsp;targetLayer)</PRE>
<DL>
<DD>Creates a synapse from every neuron in the source layer to every neuron
 in the target layer, which may be the same, with computational effort
 O(SYNAPSESTOCREATE * SYNAPSECREATIONCOST) (look in the setSynapse
 documentation to learn about the latter). Synapses that are not allowed
 will not be created. Existing synapses will be overwritten. Synapses will
 be assigned a random value dependent on the synapseInitialRange defined
 in the descriptor.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sourceLayer</CODE> - <DD><CODE>targetLayer</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="createSynapsesFromLayerToLayerWithProbability(int, int, double)"><!-- --></A><H3>
createSynapsesFromLayerToLayerWithProbability</H3>
<PRE>
public void <B>createSynapsesFromLayerToLayerWithProbability</B>(int&nbsp;sourceLayer,
                                                          int&nbsp;targetLayer,
                                                          double&nbsp;probability)</PRE>
<DL>
<DD>Creates a synapse from every neuron in the source layer to every neuron
 in the target layer (which may be the same) -- but only with a given
 probability and with computational effort O(SYNAPSESTOCREATE *
 SYNAPSECREATIONCOST) (look in the setSynapse documentation to learn about
 the latter). Synapses that are not allowed will not be created. Existing
 synapses from sourceLayer to targetLayer are not deleted prior to this.
 If you want to do so, use the removeSynapsesFromLayerToLayer method.
 However, existing synapses may be overwritten by new ones. New synapses
 will be assigned a random value dependent on the synapseInitialRange
 defined in the descriptor.
 
 <p>
 An example where to use this method is if you want to create sparse
 dynamic reservoirs to create echo state networks.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sourceLayer</CODE> - <DD><CODE>targetLayer</CODE> - <DD><CODE>probability</CODE> - must be between 0 and 1, inclusive</DL>
</DD>
</DL>
<HR>

<A NAME="removeSynapsesFromLayerToLayer(int, int)"><!-- --></A><H3>
removeSynapsesFromLayerToLayer</H3>
<PRE>
public void <B>removeSynapsesFromLayerToLayer</B>(int&nbsp;sourceLayer,
                                           int&nbsp;targetLayer)</PRE>
<DL>
<DD>Removes synapse that point from every neuron in the source layer to every
 neuron in the target layer, which may be the same, with computational
 effort O(SYNAPSESTOREMOVE * SYNAPSECREATIONCOST) (look in the setSynapse
 documentation to learn about the latter).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>sourceLayer</CODE> - <DD><CODE>targetLayer</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="getSynapseFeedForwardNonExistingMostImportantNonConvergentMethod(com.dkriesel.snipe.training.TrainingSampleLesson)"><!-- --></A><H3>
getSynapseFeedForwardNonExistingMostImportantNonConvergentMethod</H3>
<PRE>
public int[] <B>getSynapseFeedForwardNonExistingMostImportantNonConvergentMethod</B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;set)</PRE>
<DL>
<DD>Searches the most important of all not existing feed forward synapses,
 using the nonconvergent method of Finnoff 1993.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>set</CODE> - the trainingset to measure
<DT><B>Returns:</B><DD>the most important not existing feed forward connection according
         to finnoff's nonconvergent method</DL>
</DD>
</DL>
<HR>

<A NAME="countNonExistentAllowedSynapses()"><!-- --></A><H3>
countNonExistentAllowedSynapses</H3>
<PRE>
public int <B>countNonExistentAllowedSynapses</B>()</PRE>
<DL>
<DD>Returns the number of non existent, even though allowed synapses with
 computational effort in
 O(ALLOWEDSYNAPSES*LOG(SYNAPSES)+NOTALLOWEDSYNAPSES) worst case. Could
 probably be optimized once I have time to do this.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the number of non existent, even though allowed synapses.</DL>
</DD>
</DL>
<HR>

<A NAME="setNeuronBehavior(int, com.dkriesel.snipe.neuronbehavior.NeuronBehavior)"><!-- --></A><H3>
setNeuronBehavior</H3>
<PRE>
public void <B>setNeuronBehavior</B>(int&nbsp;neuron,
                              <A HREF="../../../../com/dkriesel/snipe/neuronbehavior/NeuronBehavior.html" title="interface in com.dkriesel.snipe.neuronbehavior">NeuronBehavior</A>&nbsp;behavior)</PRE>
<DL>
<DD>Assigns a Neuron Behavior to a neuron. Note that the behavior instance is
 not cloned. The neuron is assigned the very same instance given as an
 argument at time of method invocation.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - <DD><CODE>behavior</CODE> - </DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronBehavior(int)"><!-- --></A><H3>
getNeuronBehavior</H3>
<PRE>
public <A HREF="../../../../com/dkriesel/snipe/neuronbehavior/NeuronBehavior.html" title="interface in com.dkriesel.snipe.neuronbehavior">NeuronBehavior</A> <B>getNeuronBehavior</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Gets a Neuron Behavior from a neuron. Note that the behavior instance is
 not cloned. This method returns the very same behavior instance assigned
 to the given neuron. This method can for example be used to edit
 particular neuron behaviors, like changing neuron locations in Radial
 Basis Function Networks.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - 
<DT><B>Returns:</B><DD>the behavior of the given neuron</DL>
</DD>
</DL>
<HR>

<A NAME="isNeuronInput(int)"><!-- --></A><H3>
isNeuronInput</H3>
<PRE>
public boolean <B>isNeuronInput</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Checks whether the given neuron is an input neuron with computational
 effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - 
<DT><B>Returns:</B><DD>if the given neuron is an input neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="isSynapseAllowed(int, int)"><!-- --></A><H3>
isSynapseAllowed</H3>
<PRE>
public boolean <B>isSynapseAllowed</B>(int&nbsp;fromNeuron,
                                int&nbsp;toNeuron)</PRE>
<DL>
<DD>Checks whether a synapse is allowed with computational effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>fromNeuron</CODE> - <DD><CODE>toNeuron</CODE> - 
<DT><B>Returns:</B><DD>if the synapse from the neuron to the other neuron is allowed.</DL>
</DD>
</DL>
<HR>

<A NAME="getLayerOfNeuron(int)"><!-- --></A><H3>
getLayerOfNeuron</H3>
<PRE>
public int <B>getLayerOfNeuron</B>(int&nbsp;neuronNumber)</PRE>
<DL>
<DD>Returns the layer number from a neuron with computational effort in
 O(LAYERS). Remember the Neuron Indices: The first neuron is the bias
 neuron, after that all input neurons (layer 0) are enumerated, after that
 layer 1,2,... and so on. The last layer is the output layer.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuronNumber</CODE> - 
<DT><B>Returns:</B><DD>the layer number of the given Neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="countLayers()"><!-- --></A><H3>
countLayers</H3>
<PRE>
public int <B>countLayers</B>()</PRE>
<DL>
<DD>Returns the number of layers of this network with computational effort in
 O(1). Layer 0 is the input layer, the return of this function decreased
 by 1 is the output layer.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the number of layers</DL>
</DD>
</DL>
<HR>

<A NAME="countNeurons()"><!-- --></A><H3>
countNeurons</H3>
<PRE>
public int <B>countNeurons</B>()</PRE>
<DL>
<DD>Returns the number of all neurons in this neural network with
 computational effort in O(1). Doesn't count the bias.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the number of all neurons</DL>
</DD>
</DL>
<HR>

<A NAME="createNeuronInRandomLayer()"><!-- --></A><H3>
createNeuronInRandomLayer</H3>
<PRE>
public int <B>createNeuronInRandomLayer</B>()</PRE>
<DL>
<DD>Adds a neuron to the end of a random inner layer uniformly chosen with
 computational effort in O(NEURONS+SYNAPSES) (worst case). No synapses are
 added, thus the neuron remains unconnected.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the number of the new neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="mutateTopologySplitSynapseAndAddNeuron(int, int)"><!-- --></A><H3>
mutateTopologySplitSynapseAndAddNeuron</H3>
<PRE>
public int <B>mutateTopologySplitSynapseAndAddNeuron</B>(int&nbsp;i,
                                                  int&nbsp;k)</PRE>
<DL>
<DD>Splits a synapse from neuron i to neuron k by removing it and inserting
 another neuron j and two synapses from i to j and from j to k. This
 structural mutation operator is proposed in [Stanley02].
 
 <p>
 If the Neural Network consists of only 2 layers (input and output), an
 exception is thrown for neurons cannot be added to any of those layers.
 
 <p>
 The new neuron will be added to the layer of k. If k is an output neuron,
 it will be added to the layer before the layer of k.
 
 <p>
 Let x be the synaptic weight of the former synapse from i to k. Then the
 synapse from i to j is given the weight 1, and that from j to k is given
 the weight x.
 
 <p>
 This mutation operator induces a further nonlinearity, but keeps the
 behavioral gap reasonably small.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>i</CODE> - neuron i index<DD><CODE>k</CODE> - neuron k index
<DT><B>Returns:</B><DD>the index of the neuron j that was added</DL>
</DD>
</DL>
<HR>

<A NAME="mutateTopologySplitNeuron(int)"><!-- --></A><H3>
mutateTopologySplitNeuron</H3>
<PRE>
public int <B>mutateTopologySplitNeuron</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Splits an inner Neuron according to the cell division process in
 [Odri93].
 
 <p>
 In order to do this, the given inner neuron x is split in two neurons y
 and z, in a way that keeps a strong behavioral link between x and (y and
 z).
 
 <p>
 The new neurons y and z get the same incident synapses as x. Incoming
 synapses' weights are left untouched, and outgoing synapses' weights are
 multiplied by (1+alpha) for neuron y and by (-alpha) for neuron z.
 
 <p>
 The parameter alpha is chosen uniformly random between 0 and 1 for each
 invocation of this method.
 
 <p>
 Special case: Self Connections of the neuron to split will be reproduced
 as self connections at the new neuron also.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - the neuron to split.
<DT><B>Returns:</B><DD>the index of the neuron that was created.</DL>
</DD>
</DL>
<HR>

<A NAME="createNeuronInLayer(int)"><!-- --></A><H3>
createNeuronInLayer</H3>
<PRE>
public int <B>createNeuronInLayer</B>(int&nbsp;layer)</PRE>
<DL>
<DD>Adds a neuron to the end of a given layer with computational effort in
 O(NEURONS+SYNAPSES) (worst case). No Synapses are added, thus the neuron
 remains unconnected.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>layer</CODE> - 
<DT><B>Returns:</B><DD>the number of the new neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="removeNeuron(int)"><!-- --></A><H3>
removeNeuron</H3>
<PRE>
public void <B>removeNeuron</B>(int&nbsp;neuronToRemove)</PRE>
<DL>
<DD>Removes a neuron at the given position as well as all incident synapses
 with computational effort in O(NEURONS+SYNAPSES) (worst case) if the
 neuron is unconnected, otherwise with additional synapse removal effort.
 To learn about synapse removal effort, read the doc of removeSynapse.
 Updates the internal data structures. However, you can't remove neurons
 in input and output layers.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuronToRemove</CODE> - the neuron to remove</DL>
</DD>
</DL>
<HR>

<A NAME="countNeuronsInner()"><!-- --></A><H3>
countNeuronsInner</H3>
<PRE>
public int <B>countNeuronsInner</B>()</PRE>
<DL>
<DD>Returns the number of inner neurons with computational effort in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the numberOfInnerNeurons</DL>
</DD>
</DL>
<HR>

<A NAME="countNeuronsOutput()"><!-- --></A><H3>
countNeuronsOutput</H3>
<PRE>
public int <B>countNeuronsOutput</B>()</PRE>
<DL>
<DD>Just calls the descriptor's getNumberOfOutputNeurons() and returns the
 result with computational effort in O(1). This method exists only for
 convenience.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the number of output neurons</DL>
</DD>
</DL>
<HR>

<A NAME="countNeuronsInput()"><!-- --></A><H3>
countNeuronsInput</H3>
<PRE>
public int <B>countNeuronsInput</B>()</PRE>
<DL>
<DD>Just calls the descriptor's getNumberOfInputNeurons() and returns the
 result with computational effort in O(1). This method exists only for
 convenience.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the number of input neurons</DL>
</DD>
</DL>
<HR>

<A NAME="getActivation(int)"><!-- --></A><H3>
getActivation</H3>
<PRE>
public double <B>getActivation</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Returns the last activation value of a given neuron.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - 
<DT><B>Returns:</B><DD>the activation for the given neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="setActivation(int, double)"><!-- --></A><H3>
setActivation</H3>
<PRE>
public void <B>setActivation</B>(int&nbsp;neuron,
                          double&nbsp;activation)</PRE>
<DL>
<DD>Sets the value that will be used as the last activation value of a given
 neuron. Nice feature for some network topologies, e.g. hopfield networks.
 If you do this, be absolutely sure what you are doing.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - <DD><CODE>activation</CODE> - the activation for the given neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="getNetInput(int)"><!-- --></A><H3>
getNetInput</H3>
<PRE>
public double <B>getNetInput</B>(int&nbsp;neuron)</PRE>
<DL>
<DD>Returns the last net input value of a given neuron.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>neuron</CODE> - 
<DT><B>Returns:</B><DD>the net input for the given neuron.</DL>
</DD>
</DL>
<HR>

<A NAME="importFromString(java.lang.String)"><!-- --></A><H3>
importFromString</H3>
<PRE>
public void <B>importFromString</B>(java.lang.String&nbsp;description)
                      throws java.lang.Exception</PRE>
<DL>
<DD><p>
 Parses a Neural Net String built by the exportFromString method and
 builds up the neural net contained. The old neural network data will be
 discarded.
 
 <p>
 NOTE: This function includes several SetSynapse calls and thus is not
 optimal in calculating complexity, even though it is not too slow either.
 
 <p>
 IMPORTANT: Synapses that are not allowed from the neural network
 descriptor will not be created and no exception will be thrown. If
 furthermore the parsed string contains a different number of layers,
 inputs, or outputs than defined in the layer, an exception will be thrown
 and nothing will be discarded.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>description</CODE> - 
<DT><B>Throws:</B>
<DD><CODE>java.lang.Exception</CODE> - if number of layers, inputs, or outputs is different to those
             in the descriptor or another parsing error occurs</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronInnerLeastImportantNonConvergentMethod(com.dkriesel.snipe.training.TrainingSampleLesson)"><!-- --></A><H3>
getNeuronInnerLeastImportantNonConvergentMethod</H3>
<PRE>
public int <B>getNeuronInnerLeastImportantNonConvergentMethod</B>(<A HREF="../../../../com/dkriesel/snipe/training/TrainingSampleLesson.html" title="class in com.dkriesel.snipe.training">TrainingSampleLesson</A>&nbsp;set)</PRE>
<DL>
<DD>Searches through all inner neurons, which is the least important
 according to the sum of incident synapse importance using the
 nonconvergent method of Finnoff 1993. Returns the inner neuron index
 found with smallest sum of synapse importance or -1 if there is no
 synapse respective no inner neuron.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>set</CODE> - the training set
<DT><B>Returns:</B><DD>the neuron index</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronInnerLeastConnected()"><!-- --></A><H3>
getNeuronInnerLeastConnected</H3>
<PRE>
public int <B>getNeuronInnerLeastConnected</B>()</PRE>
<DL>
<DD>Returns the inner neuron with the smallest absolute sum of incident
 synapse weights with computational effort in O(SYNAPSES).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the inner neuron with the smallest absolute sum of incident
         synapse weights.</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronInnerLeastSending()"><!-- --></A><H3>
getNeuronInnerLeastSending</H3>
<PRE>
public int <B>getNeuronInnerLeastSending</B>()</PRE>
<DL>
<DD>Returns the inner neuron with the smallest absolute sum of outgoing
 synapse weights with computational effort in O(SYNAPSES).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the inner neuron with the smallest absolute sum of outgoing
         synapse weights.</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronInnerLeastReceiving()"><!-- --></A><H3>
getNeuronInnerLeastReceiving</H3>
<PRE>
public int <B>getNeuronInnerLeastReceiving</B>()</PRE>
<DL>
<DD>Returns the inner neuron with the smallest absolute sum of incoming
 synapse weights with computational effort in O(SYNAPSES).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the inner neuron with the smallest absolute sum of incoming
         synapse weights.</DL>
</DD>
</DL>
<HR>

<A NAME="exportToString()"><!-- --></A><H3>
exportToString</H3>
<PRE>
public java.lang.String <B>exportToString</B>()</PRE>
<DL>
<DD>This method returns a string description of this neural network that in
 turn can be parsed by the importFromString method. It saves the topology,
 namely number of neurons in every layer, synapses and synaptic weights.
 No activation and neuron behavior information is stored.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>a string describing the neural network topology.</DL>
</DD>
</DL>
<HR>

<A NAME="countNeuronsInLayer(int)"><!-- --></A><H3>
countNeuronsInLayer</H3>
<PRE>
public int <B>countNeuronsInLayer</B>(int&nbsp;layer)</PRE>
<DL>
<DD>Returns the number of neurons in a given layer with computational effort
 in O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>layer</CODE> - 
<DT><B>Returns:</B><DD>the number of neurons in a given layer.</DL>
</DD>
</DL>
<HR>

<A NAME="countSynapses()"><!-- --></A><H3>
countSynapses</H3>
<PRE>
public int <B>countSynapses</B>()</PRE>
<DL>
<DD>Counts all existing synapses with expected computational effort in O(1).
 Exact computational effort is in O(EPSILON*NEURONS), where EPSILON is
 very small.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the numberOfExistingSynapses</DL>
</DD>
</DL>
<HR>

<A NAME="getNeuronInnerRandom()"><!-- --></A><H3>
getNeuronInnerRandom</H3>
<PRE>
public int <B>getNeuronInnerRandom</B>()</PRE>
<DL>
<DD>Uniformly selects a random inner neuron with computational effort in
 O(1).
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>a uniformly selected inner neuron index or -1 if none exists.</DL>
</DD>
</DL>
<HR>

<A NAME="toString()"><!-- --></A><H3>
toString</H3>
<PRE>
public java.lang.String <B>toString</B>()</PRE>
<DL>
<DD>Returns a very simple string representation just printing out the neurons
 per layer structure of the neural net and the number of synapses.
<P>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE>toString</CODE> in class <CODE>java.lang.Object</CODE></DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the string representation</DL>
</DD>
</DL>
<HR>

<A NAME="hashCode()"><!-- --></A><H3>
hashCode</H3>
<PRE>
public int <B>hashCode</B>()</PRE>
<DL>
<DD>Hashes the neural network and returns the hash.
<P>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE>hashCode</CODE> in class <CODE>java.lang.Object</CODE></DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>hash value</DL>
</DD>
</DL>
<HR>

<A NAME="getDescriptor()"><!-- --></A><H3>
getDescriptor</H3>
<PRE>
public <A HREF="../../../../com/dkriesel/snipe/core/NeuralNetworkDescriptor.html" title="class in com.dkriesel.snipe.core">NeuralNetworkDescriptor</A> <B>getDescriptor</B>()</PRE>
<DL>
<DD>Returns the NeuralNetworkDescriptor instance that was used to create this
 NeuralNetwork.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the descriptor</DL>
</DD>
</DL>
<HR>

<A NAME="clone()"><!-- --></A><H3>
clone</H3>
<PRE>
public <A HREF="../../../../com/dkriesel/snipe/core/NeuralNetwork.html" title="class in com.dkriesel.snipe.core">NeuralNetwork</A> <B>clone</B>()</PRE>
<DL>
<DD>Performs a complete deep copy of this Neural Network and returns it.
 Every data structures, caches and shadows are cloned.
<P>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE>clone</CODE> in class <CODE>java.lang.Object</CODE></DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the clone of this object</DL>
</DD>
</DL>
<HR>

<A NAME="equals(java.lang.Object)"><!-- --></A><H3>
equals</H3>
<PRE>
public boolean <B>equals</B>(java.lang.Object&nbsp;obj)</PRE>
<DL>
<DD>Checks if this object equals another. If the other is a neural network
 too, it is checked if the four primary data storage arrays equal each
 other. If so, true is returned, otherwise false.
<P>
<DD><DL>
<DT><B>Overrides:</B><DD><CODE>equals</CODE> in class <CODE>java.lang.Object</CODE></DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>obj</CODE> - the object to check</DL>
</DD>
</DL>
<HR>

<A NAME="getResilientBackpropagationDeltaZero()"><!-- --></A><H3>
getResilientBackpropagationDeltaZero</H3>
<PRE>
public double <B>getResilientBackpropagationDeltaZero</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the resilientBackpropagationDeltaZero</DL>
</DD>
</DL>
<HR>

<A NAME="setResilientBackpropagationDeltaZero(double)"><!-- --></A><H3>
setResilientBackpropagationDeltaZero</H3>
<PRE>
public void <B>setResilientBackpropagationDeltaZero</B>(double&nbsp;resilientBackpropagationDeltaZero)</PRE>
<DL>
<DD>Sets the resilientBackpropagationDeltaZero. Default Value is 0.1.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>resilientBackpropagationDeltaZero</CODE> - the resilientBackpropagationDeltaZero to set</DL>
</DD>
</DL>
<HR>

<A NAME="getResilientBackpropagationDeltaMax()"><!-- --></A><H3>
getResilientBackpropagationDeltaMax</H3>
<PRE>
public double <B>getResilientBackpropagationDeltaMax</B>()</PRE>
<DL>
<DD>*
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the resilientBackpropagationDeltaMax</DL>
</DD>
</DL>
<HR>

<A NAME="setResilientBackpropagationDeltaMax(double)"><!-- --></A><H3>
setResilientBackpropagationDeltaMax</H3>
<PRE>
public void <B>setResilientBackpropagationDeltaMax</B>(double&nbsp;resilientBackpropagationDeltaMax)</PRE>
<DL>
<DD>Sets the resilientBackpropagationDeltaMax. Default value is 50.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>resilientBackpropagationDeltaMax</CODE> - the resilientBackpropagationDeltaMax to set</DL>
</DD>
</DL>
<HR>

<A NAME="getResilientBackpropagationDeltaMin()"><!-- --></A><H3>
getResilientBackpropagationDeltaMin</H3>
<PRE>
public double <B>getResilientBackpropagationDeltaMin</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the resilientBackpropagationDeltaMin</DL>
</DD>
</DL>
<HR>

<A NAME="setResilientBackpropagationDeltaMin(double)"><!-- --></A><H3>
setResilientBackpropagationDeltaMin</H3>
<PRE>
public void <B>setResilientBackpropagationDeltaMin</B>(double&nbsp;resilientBackpropagationDeltaMin)</PRE>
<DL>
<DD>Sets the resilientBackpropagationDeltaMin. Default value is 0.000001.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>resilientBackpropagationDeltaMin</CODE> - the resilientBackpropagationDeltaMin to set</DL>
</DD>
</DL>
<HR>

<A NAME="getResilientBackpropagationEtaMinus()"><!-- --></A><H3>
getResilientBackpropagationEtaMinus</H3>
<PRE>
public double <B>getResilientBackpropagationEtaMinus</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the resilientBackpropagationEtaMinus</DL>
</DD>
</DL>
<HR>

<A NAME="setResilientBackpropagationEtaMinus(double)"><!-- --></A><H3>
setResilientBackpropagationEtaMinus</H3>
<PRE>
public void <B>setResilientBackpropagationEtaMinus</B>(double&nbsp;resilientBackpropagationEtaMinus)</PRE>
<DL>
<DD>Sets the resilientBackpropagationEtaMinus. Default value is 0.5.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>resilientBackpropagationEtaMinus</CODE> - the resilientBackpropagationEtaMinus to set</DL>
</DD>
</DL>
<HR>

<A NAME="getResilientBackpropagationEtaPlus()"><!-- --></A><H3>
getResilientBackpropagationEtaPlus</H3>
<PRE>
public double <B>getResilientBackpropagationEtaPlus</B>()</PRE>
<DL>
<DD><DL>
</DL>
</DD>
<DD><DL>

<DT><B>Returns:</B><DD>the resilientBackpropagationEtaPlus</DL>
</DD>
</DL>
<HR>

<A NAME="setResilientBackpropagationEtaPlus(double)"><!-- --></A><H3>
setResilientBackpropagationEtaPlus</H3>
<PRE>
public void <B>setResilientBackpropagationEtaPlus</B>(double&nbsp;resilientBackpropagationEtaPlus)</PRE>
<DL>
<DD>Sets the resilientBackpropagationEtaPlus. Default Value is 1.2.
<P>
<DD><DL>
</DL>
</DD>
<DD><DL>
<DT><B>Parameters:</B><DD><CODE>resilientBackpropagationEtaPlus</CODE> - the resilientBackpropagationEtaPlus to set</DL>
</DD>
</DL>
<!-- ========= END OF CLASS DATA ========= -->
<HR>


<!-- ======= START OF BOTTOM NAVBAR ====== -->
<A NAME="navbar_bottom"><!-- --></A>
<A HREF="#skip-navbar_bottom" title="Skip navigation links"></A>
<TABLE BORDER="0" WIDTH="100%" CELLPADDING="1" CELLSPACING="0" SUMMARY="">
<TR>
<TD COLSPAN=2 BGCOLOR="#EEEEFF" CLASS="NavBarCell1">
<A NAME="navbar_bottom_firstrow"><!-- --></A>
<TABLE BORDER="0" CELLPADDING="0" CELLSPACING="3" SUMMARY="">
  <TR ALIGN="center" VALIGN="top">
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../overview-summary.html"><FONT CLASS="NavBarFont1"><B>Overview</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-summary.html"><FONT CLASS="NavBarFont1"><B>Package</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#FFFFFF" CLASS="NavBarCell1Rev"> &nbsp;<FONT CLASS="NavBarFont1Rev"><B>Class</B></FONT>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="class-use/NeuralNetwork.html"><FONT CLASS="NavBarFont1"><B>Use</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="package-tree.html"><FONT CLASS="NavBarFont1"><B>Tree</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../deprecated-list.html"><FONT CLASS="NavBarFont1"><B>Deprecated</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../index-files/index-1.html"><FONT CLASS="NavBarFont1"><B>Index</B></FONT></A>&nbsp;</TD>
  <TD BGCOLOR="#EEEEFF" CLASS="NavBarCell1">    <A HREF="../../../../help-doc.html"><FONT CLASS="NavBarFont1"><B>Help</B></FONT></A>&nbsp;</TD>
  </TR>
</TABLE>
</TD>
<TD ALIGN="right" VALIGN="top" ROWSPAN=3><EM>
</EM>
</TD>
</TR>

<TR>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
&nbsp;PREV CLASS&nbsp;
&nbsp;<A HREF="../../../../com/dkriesel/snipe/core/NeuralNetworkDescriptor.html" title="class in com.dkriesel.snipe.core"><B>NEXT CLASS</B></A></FONT></TD>
<TD BGCOLOR="white" CLASS="NavBarCell2"><FONT SIZE="-2">
  <A HREF="../../../../index.html?com/dkriesel/snipe/core/NeuralNetwork.html" target="_top"><B>FRAMES</B></A>  &nbsp;
&nbsp;<A HREF="NeuralNetwork.html" target="_top"><B>NO FRAMES</B></A>  &nbsp;
&nbsp;<SCRIPT type="text/javascript">
  <!--
  if(window==top) {
    document.writeln('<A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>');
  }
  //-->
</SCRIPT>
<NOSCRIPT>
  <A HREF="../../../../allclasses-noframe.html"><B>All Classes</B></A>
</NOSCRIPT>


</FONT></TD>
</TR>
<TR>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
  SUMMARY:&nbsp;NESTED&nbsp;|&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_summary">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_summary">METHOD</A></FONT></TD>
<TD VALIGN="top" CLASS="NavBarCell3"><FONT SIZE="-2">
DETAIL:&nbsp;FIELD&nbsp;|&nbsp;<A HREF="#constructor_detail">CONSTR</A>&nbsp;|&nbsp;<A HREF="#method_detail">METHOD</A></FONT></TD>
</TR>
</TABLE>
<A NAME="skip-navbar_bottom"></A>
<!-- ======== END OF BOTTOM NAVBAR ======= -->

<HR>

</BODY>
</HTML>
